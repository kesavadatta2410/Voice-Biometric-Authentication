# ğŸ™ï¸ Authentication using Voice Biometrics

A deep-learning project for **text-independent speaker verification** using Siamese Networks. Three distinct approaches are implemented with increasing architectural sophistication â€” from a custom CNN+GRU network, to VGGish transfer learning, to a full production-grade **ECAPA-TDNN** with Online Triplet Loss.

---

## ğŸ“‘ Table of Contents

- [Overview](#overview)
- [Project Structure](#project-structure)
- [Approaches](#approaches)
  - [Approach 1 â€” Custom CNN + GRU Siamese Network](#approach-1--custom-cnn--gru-siamese-network)
  - [Approach 2 â€” VGGish-Based Siamese Network (Transfer Learning)](#approach-2--vggish-based-siamese-network-transfer-learning)
  - [Approach 3 â€” ECAPA-TDNN with Online Triplet Loss â­](#approach-3--ecapa-tdnn-with-online-triplet-loss-)
- [Dataset Structure](#dataset-structure)
- [Installation](#installation)
- [How to Run Approach 3](#how-to-run-approach-3)
- [Data Path Reference](#data-path-reference)
- [Evaluation](#evaluation)
- [Results](#results)
- [Future Work](#future-work)

---

## Overview

This project tackles the **speaker verification** problem: given two audio clips, determine whether they were uttered by the **same person**. This is a core task in voice-based biometric authentication systems.

All approaches use a **Siamese Network** architecture that maps voice recordings into a compact embedding space where:
- Embeddings of the **same speaker** are pulled **close together**
- Embeddings of **different speakers** are pushed **far apart**

---

## Project Structure

### Approach 1 â€” Custom CNN + GRU

| File | Contents |
|---|---|
| `preprocess.py` | Audio trimming, normalisation, resampling |
| `feature_extraction.py` | 80-band log-mel spectrogram extraction |
| `speaker_pairup.py` | On-the-fly speaker pair generation |
| `model.py` | SiameseNetwork: CNNEncoder + GRU + ProjectionHead |
| `loss.py` | Contrastive Loss (cosine distance) |
| `train.py` | Training loop with best-model checkpointing |
| `evaluate.py` | EER computation and optimal threshold |

### Approach 2 â€” VGGish Transfer Learning

| File | Contents |
|---|---|
| `dataset_pairup.py` | On-the-fly speaker pair generation |
| `model.py` | SiameseNet: frozen VGGish features + projection head |
| `loss.py` | Contrastive Loss |
| `train.py` | Training loop (AdamW, lr=1e-4) |
| `training_with_early_stopping.py` | Training variant with early stopping |
| `evaluate.py` | EER + similarity score distribution plot |

### Approach 3 â€” ECAPA-TDNN â­

| File | Contents |
|---|---|
| `config.yaml` | Centralised hyperparameters |
| `preprocess.py` | Waveform loading, VAD trim, torchaudio normalisation |
| `feature_extraction.py` | `LogMelExtractor` â€” 80-band, instance-normalised |
| `dataset_pairup.py` | `SpeakerPairDataset` â€” speaker-disjoint splits, SpecAugment (80%), 50/50 balance |
| `model.py` | ECAPA-TDNN backbone, SpeechBrain 1.0+ pretrained weights, dropout=0.5, expanded projection head |
| `loss.py` | `OnlineTripletLoss` (semi-hard mining), `ContrastiveLoss`, `TripletLoss` |
| `train.py` | GPU training loop â€” AMP, AdamW (wd=0.01), OneCycleLR, early stopping (patience=15), per-epoch JSON log |
| `evaluate.py` | EER, minDCF, DET curve, similarity distribution, t-SNE embeddings |
| `diagnose_data.py` | Data quality audit â€” speaker overlap, balance, length distribution |
| `diagnose_model.py` | Model audit â€” embedding norms, gradient flow, pretrained weight loading |
| `diagnose_training.py` | Training dynamics â€” loss curves, EER over epochs, overfitting detection |
| `quick_test.py` | 5-epoch ablation test to verify pipeline end-to-end |

#### Execution Order for Approach 3

| Step | Script | Command |
|---|---|---|
| 1 | `diagnose_data.py` | `python Approach3/diagnose_data.py Data --out_dir Approach3/diagnostics` |
| 2 | `diagnose_model.py` | `python Approach3/diagnose_model.py --out_dir Approach3/diagnostics` |
| 3 | `quick_test.py` *(optional)* | `python Approach3/quick_test.py Data --fix all --epochs 5` |
| 4 | `train.py` | `python Approach3/train.py Data --save_dir Approach3/checkpoints --epochs 100` |
| 5 | `diagnose_training.py` *(optional)* | `python Approach3/diagnose_training.py --log Approach3/checkpoints/train_log.csv --out_dir Approach3/diagnostics` |
| 6 | `evaluate.py` | `python Approach3/evaluate.py Approach3/checkpoints/trials.tsv Approach3/checkpoints/best_model.pth --results_dir Approach3/results` |

### Root

| File / Folder | Contents |
|---|---|
| `Data/` | Raw audio files (~1,440 utterances, 24 speakers) |
| `Approach3/checkpoints/` | `best_model.pth`, `latest_model.pth`, `train_log.csv`, `epoch_log.json` |
| `Approach3/results/` | `eer_report.txt`, `results.json`, `det_curve.png`, `similarity_distribution.png`, `tsne_embeddings.png` |
| `siamese_vggish*.pth` | Saved VGGish Siamese model weights |
| `utils.ipynb` | Utility notebook for exploration |
| `pyproject.toml` | Project dependencies |

---

## Approaches

### Approach 1 â€” Custom CNN + GRU Siamese Network

A fully custom architecture trained from scratch on raw mel spectrogram features.

#### Architecture

```
Input Spectrogram (1 Ã— 80 Ã— T)
    â”‚
    â–¼
CNNEncoder
  â”œâ”€â”€ Conv2d(1â†’32, 3Ã—3) + ReLU + MaxPool2d  â†’  (32 Ã— 40 Ã— T/2)
  â””â”€â”€ Conv2d(32â†’64, 3Ã—3) + ReLU + MaxPool2d â†’  (64 Ã— 20 Ã— T/4)
    â”‚
    â–¼  reshape â†’ (B, T/4, 64Ã—20)
TemporalEncoder
  â””â”€â”€ GRU(input=64Ã—20, hidden=128) â†’ last hidden state â†’ (B, 128)
    â”‚
    â–¼
ProjectionHead
  â””â”€â”€ Linear(128â†’128) + L2 Normalisation â†’ 128-D unit-sphere embedding
```

#### Training Details

| Hyperparameter | Value |
|---|---|
| Epochs | 10 |
| Batch size | 16 |
| Optimiser | Adam (lr=1e-3) |
| Loss | Contrastive Loss (cosine distance, margin=1.0) |
| Features | 80-band log-mel, N_FFT=400, hop=160 |
| Max time frames | 400 (pad/crop) |

---

### Approach 2 â€” VGGish-Based Siamese Network (Transfer Learning)

Leverages the **pretrained VGGish** model (trained on AudioSet) as a frozen feature extractor.

#### Architecture

```
Input Spectrogram (1 Ã— 64 Ã— 400)
    â”‚
    â–¼
VGGishEncoder
  â”œâ”€â”€ Frozen VGGish convolutional features   â†’  (B, 512, H, W)
  â”œâ”€â”€ AdaptiveAvgPool2d(1, 1)               â†’  (B, 512, 1, 1)
  â”œâ”€â”€ Flatten                               â†’  (B, 512)
  â””â”€â”€ Linear(512â†’128) + L2 Normalise       â†’  (B, 128)
```

#### Training Details

| Hyperparameter | Value |
|---|---|
| Epochs | 30 (early stopping variant available) |
| Batch size | 16 |
| Optimiser | Adam (lr=1e-4, trainable params only) |
| Loss | Contrastive Loss |
| Features | 64 mel bands, max 400 time frames (pad/crop) |
| Backbone | VGGish from `harritaylor/torchvggish` (frozen) |

---

### Approach 3 â€” ECAPA-TDNN with Online Triplet Loss â­

The most advanced approach, implementing the full **ECAPA-TDNN** architecture with **Online Triplet Loss** (semi-hard negative mining) â€” robust for small speaker datasets without needing a large class-weight matrix.

#### Architecture

```
Input (B, 80, T)  â† variable length, no padding required at input
    â”‚
    â–¼
ECAPA-TDNN Backbone
  â”œâ”€â”€ Conv1d(80â†’512, k=5)  +  BN  +  ReLU
  â”œâ”€â”€ SE-Res2Block (dilation=2)  â†’ e1
  â”œâ”€â”€ SE-Res2Block (dilation=3)  â†’ e2
  â”œâ”€â”€ SE-Res2Block (dilation=4)  â†’ e3
  â””â”€â”€ MFA: concat(e1,e2,e3) â†’ Conv1d(1536â†’1536)    â†’  (B, 1536, T)
    â”‚
    â–¼
Attentive Statistical Pooling
  â”œâ”€â”€ Self-attention weights (softmax over T)
  â”œâ”€â”€ Weighted mean                               â†’  (B, 1536)
  â””â”€â”€ Weighted std                                â†’  (B, 1536)
  concat â†’ (B, 3072)
    â”‚
    â–¼
Projection Head
  â”œâ”€â”€ Linear(3072â†’256) + BatchNorm1d + ReLU + Dropout(0.3)
  â””â”€â”€ Linear(256â†’128) + BatchNorm1d + L2 Normalise  â†’  (B, 128)
```

Additionally, **frame-level dropout (p=0.5)** is applied after the backbone before pooling.

**SE-Res2Block** = Squeeze-and-Excitation + Res2Net multi-scale convolution + residual connection.

#### Loss Function

```
Total Loss = OnlineTripletLoss(all_emb, speaker_labels) + 0.1 Ã— ContrastiveLoss(emb_a, emb_b, pair_labels)
```

**OnlineTripletLoss** mines semi-hard negatives within each mini-batch:

```
L = max(d(a,p) âˆ’ d(a,n) + margin, 0)    margin=0.5
```

This works well with small speaker counts (24 speakers) since no class-weight matrix is needed.

#### Training Features

| Feature | Detail |
|---|---|
| Backbone | SpeechBrain 1.0+ pretrained ECAPA-TDNN (VoxCeleb) â€” fine-tuned |
| Optimiser | AdamW (lr=**1e-4**, weight_decay=**0.01**) |
| LR Schedule | **OneCycleLR** â€” 30% linear warmup â†’ cosine decay (per-batch) |
| Precision | Mixed precision (AMP) â€” **GPU required** |
| Grad clipping | max-norm=1.0 |
| Loss | OnlineTripletLoss (margin=0.5) + 0.1 Ã— ContrastiveLoss |
| Frame dropout | p=0.5 (before pooling) |
| Augmentation | **SpecAugment 80%** â€” FreqMask(15) + TimeMask(20) on train split |
| Early stopping | Monitors validation EER, patience=**15** |
| Speaker split | 80/20 â€” **strictly speaker-disjoint** (checked at 3 levels) |
| Input | Dynamic length; padded per-batch by `collate_fn` |
| Epochs | Up to **100** |
| Batch size | **64** |
| Pairs/epoch | 10,000 (50% positive / 50% negative, enforced) |
| Pre-flight checks | Asserts speaker disjointness, embedding norms |
| Checkpoints | `best_model.pth` + `latest_model.pth` |
| Logging | `train_log.csv` + `epoch_log.json` (per-epoch metrics) |

---

## Dataset Structure

All approaches expect raw audio organised as **speaker-ID folders**:

```
Data/
â””â”€â”€ raw/
    â”œâ”€â”€ speaker_0001/
    â”‚   â”œâ”€â”€ utterance_01.wav
    â”‚   â”œâ”€â”€ utterance_02.wav
    â”‚   â””â”€â”€ ...
    â”œâ”€â”€ speaker_0002/
    â”‚   â””â”€â”€ ...
    â””â”€â”€ ...
```

> **Each speaker folder must have at least 2 utterances** for pair sampling to work.

For Approach 1 & 2, preprocessed features are saved as `.npy` files:
```
data/
â”œâ”€â”€ processed/   â† resampled WAV files (from Approach1/preprocess.py)
â””â”€â”€ features/    â† .npy log-mel spectrograms
```

For **Approach 3**, feature extraction is done on-the-fly â€” no pre-saved `.npy` files are needed.

---

## Installation

### With uv (recommended)

```bash
uv sync
```

### With pip

```bash
pip install torch torchvision torchaudio librosa scikit-learn numpy matplotlib \
            soundfile noisereduce pedalboard speechbrain tqdm seaborn scipy
```


---

### Dataset layout expected

Approach 3 reads raw `.wav` files directly â€” no preprocessing step needed.

```
Data/
â”œâ”€â”€ Actor_01/
â”‚   â”œâ”€â”€ 03-01-01-01-01-01-01.wav
â”‚   â””â”€â”€ ...
â”œâ”€â”€ Actor_02/
â”‚   â””â”€â”€ ...
â””â”€â”€ ...  (24 actors / speakers)
```

Each speaker folder must contain **â‰¥ 2 utterances** for positive pair sampling.

---

### Step 1 â€” Diagnose data pipeline

Checks for speaker overlap, utterance overlap, pair balance, and audio length distribution.
Saves report + plot to `Approach3/diagnostics/`.

```powershell
python Approach3/diagnose_data.py Data --out_dir Approach3/diagnostics
```

Expected output summary:
```
Total files   : 1440  |  Total speakers: 24
Speaker overlap  : âœ… PASS (0 shared speakers)
Utterance overlap: âœ… PASS
Pair balance     : âœ… PASS (~50% positive)
```

---

### Step 2 â€” Diagnose model architecture

Verifies embedding norms, gradient flow, and SpeechBrain pretrained weight loading.

```powershell
# First run (no checkpoint yet)
python Approach3/diagnose_model.py --out_dir Approach3/diagnostics

# After training (with checkpoint)
python Approach3/diagnose_model.py --checkpoint Approach3/checkpoints/best_model.pth --out_dir Approach3/diagnostics
```

---

### Step 3 â€” Quick 5-epoch ablation test _(optional but recommended)_

Runs a short 5-epoch training cycle to verify the pipeline end-to-end before committing to full training.

```powershell
python Approach3/quick_test.py Data --fix all --epochs 5
```

Expected result (RAVDESS, 24 speakers):
```
Epoch 5/5 â€” loss=0.0235  val_eer=13.25%
RESULT: final_eer = 13.25%  (target < 20%)
âœ… PASS â€” proceed to full training
```

`--fix` options: `data` | `arcface` | `pretrained` | `all`

---

### Step 4 â€” Full training

```powershell
python Approach3/train.py Data --save_dir Approach3/checkpoints --epochs 100 --batch_size 64 --lr 1e-4
```

**All arguments:**

| Argument | Default | Description |
|---|---|---|
| `data_dir` | *(required)* | Root dir â€” speaker subdirs with `.wav` files |
| `--save_dir` | `checkpoints` | Saves `best_model.pth`, `latest_model.pth`, `train_log.csv`, `epoch_log.json` |
| `--epochs` | `100` | Maximum training epochs |
| `--batch_size` | `64` | Batch size (minimum 32 recommended) |
| `--lr` | `1e-4` | Initial learning rate (for fine-tuning pretrained backbone) |
| `--triplet_margin` | `0.5` | Triplet loss margin |
| `--resume` | `None` | Path to a checkpoint to resume from |
| `--seed` | `42` | Global random seed |

Training prints per-epoch metrics and saves a CSV + JSON log:
```
[Epoch 001/100] loss=0.4821  val_eer=22.33%  val_loss=0.3012  lr=3.33e-05
[Epoch 013/100] loss=0.1953  val_eer=18.15%  val_loss=0.2764  lr=8.67e-05
  âœ“ New best EER: 18.15%
```

**Resume from checkpoint:**
```powershell
python Approach3/train.py Data --save_dir Approach3/checkpoints --epochs 100 --resume Approach3/checkpoints/latest_model.pth
```

---

### Step 5 â€” Diagnose training dynamics _(after training)_

Plots loss curves, EER over epochs, LR schedule, and detects overfitting.

```powershell
python Approach3/diagnose_training.py \
  --log Approach3/checkpoints/train_log.csv \
  --checkpoint Approach3/checkpoints/best_model.pth \
  --out_dir Approach3/diagnostics
```

---

### Step 6 â€” Evaluate

First generate a trials file (tab-separated: `path_a \t path_b \t label`, 1=same speaker, 0=different):

```powershell
# Auto-generate trials.tsv from your Data/ directory
python -c "
import random, pathlib, itertools
data = pathlib.Path('Data'); rng = random.Random(42)
spk_files = {s.name: [str(f) for ext in ('*.wav','*.flac') for f in s.glob(ext)] for s in data.iterdir() if s.is_dir()}
pairs = [(a,b,1) for spk,fs in spk_files.items() for a,b in itertools.combinations(fs[:10],2)]
negs = []
while len(negs)<len(pairs):
    s1,s2=rng.sample(list(spk_files),2); negs.append((rng.choice(spk_files[s1]),rng.choice(spk_files[s2]),0))
all_p=pairs+negs; rng.shuffle(all_p)
open('Approach3/checkpoints/trials.tsv','w').writelines(f'{a}	{b}	{l}
' for a,b,l in all_p)
print(f'{len(all_p)} pairs written')
"

python Approach3/evaluate.py Approach3/checkpoints/trials.tsv Approach3/checkpoints/best_model.pth --results_dir Approach3/results
```

**All arguments:**

| Argument | Default | Description |
|---|---|---|
| `trials_file` | *(required)* | Path to the TSV trial pairs file â€” **no header row** |
| `model_path` | *(required)* | Path to `best_model.pth` checkpoint |
| `--results_dir` | `results` | Directory for output artefacts |
| `--n_mels` | `80` | Must match training config |
| `--channels` | `512` | Must match training config |
| `--embedding_dim` | `128` | Must match training config |
| `--p_target` | `0.01` | Prior probability for minDCF |

**Output artefacts** (saved to `Approach3/results/`):

| File | Description |
|---|---|
| `eer_report.txt` | EER %, minDCF, optimal threshold, trial count |
| `results.json` | Machine-readable metrics (always up-to-date) |
| `det_curve.png` | Detection Error Tradeoff (DET) curve |
| `similarity_distribution.png` | Histogram: same vs different speaker scores |
| `tsne_embeddings.png` | t-SNE of speaker embeddings |

---

## Data Path Reference

| Approach | Script | Where to set the data path |
|---|---|---|
| **Approach 1** | `preprocess.py` | Hardcoded `RAW_DATA_DIR` â€” edit line 6 |
| **Approach 1** | `feature_extraction.py` | Hardcoded `PROCESSED_DATA_DIR` â€” edit line 5-6 |
| **Approach 1** | `train.py` | Hardcoded `FEATURES_DATA_DIR` â€” edit line 72 |
| **Approach 1** | `evaluate.py` | Hardcoded `FEATURES_DATA_DIR` â€” edit line 77 |
| **Approach 2** | `train.py` | Hardcoded in `train()` function â€” edit line 47 |
| **Approach 2** | `evaluate.py` | Hardcoded in `run_evaluation()` â€” edit line 18 |
| **Approach 3** | `train.py` | âœ… CLI argument `data_dir` â€” no edits needed |
| **Approach 3** | `evaluate.py` | âœ… CLI arguments `trials_file` + `model_path` â€” no edits needed |

> âš ï¸ **Approach 1 and 2 have Linux-style hardcoded paths** (`/home/rohithkaki/...`). Before running these, update the path constants at the top of each script to point to your local `Data/` directory.

**For Approach 1 & 2 on Windows, replace paths like:**
```python
# Old (Linux path)
FEATURES_DATA_DIR = "/home/rohithkaki/Voice_Biometrics/data/features"

# New (Windows path â€” use raw string or forward slashes)
FEATURES_DATA_DIR = r"D:\Authentication_using_Voice_Biometrics-main\Data\features"
```

---

## Evaluation Metrics

| Metric | Description |
|---|---|
| **EER** (Equal Error Rate) | Point where FAR = FRR. **Lower is better.** |
| **minDCF** | Minimum Detection Cost Function. **Lower is better.** |
| **Optimal Threshold** | Cosine similarity threshold at the EER operating point |

---

## Results

| | Approach 1 (CNN+GRU) | Approach 2 (VGGish) | Approach 3 (ECAPA-TDNN) |
|---|---|---|---|
| Backbone | Trained from scratch | Pretrained VGGish (frozen) | **Pretrained SpeechBrain ECAPA-TDNN** |
| Input features | 80-band log-mel | 64-band mel | 80-band log-mel (instance norm) |
| Input handling | Fixed 400 frames | Fixed 400 frames | **Dynamic length** |
| Loss | Contrastive | Contrastive | **OnlineTripletLoss (margin=0.5) + Contrastive** |
| Augmentation | None | None | **SpecAugment 80%** (FreqMask+TimeMask) |
| GPU required | No | No | **Yes (CUDA)** |
| Epochs | 10 | 30 | Up to 100 (early stopping, patience=15) |
| Embedding dim | 128 | 128 | 128 |
| Val EER (15 epochs) | â€” | â€” | **18.15%** |
| **Eval EER** (2160 trials) | â€” | â€” | **âœ… 5.83%** |
| **minDCF** (p=0.01) | â€” | â€” | **0.2731** |
| **Threshold** | â€” | â€” | **0.699** |
| Metrics | EER | EER | **EER + minDCF + DET curve** |
| Plots | â€” | Score distribution | DET, score dist., t-SNE |

Training loss curve: [`loss_plot.png`](loss_plot.png)  
Result images: [`Results_images/`](Results_images/)

---

## Future Work

| Feature | Status |
|---|---|
| Pretrained backbone (SpeechBrain 1.0+) | âœ… Done |
| Speaker-disjoint splits | âœ… Enforced at 3 levels |
| Online Triplet Loss (semi-hard mining) | âœ… Done |
| SpecAugment data augmentation (80%) | âœ… Done |
| OneCycleLR scheduler | âœ… Done |
| Per-epoch JSON logging | âœ… `epoch_log.json` |
| Dropout regularisation (p=0.5) | âœ… Frame + projection head |
| Hard negative mining (embedding pool) | ğŸ”² Planned |
| Dataset scale | ğŸ”² VoxCeleb1/2 (1000+ speakers) |
| SubCenterArcFace | ğŸ”² Planned |
| VoxCeleb1-H benchmark evaluation | ğŸ”² Planned |

---

## References

- [VGGish](https://github.com/harritaylor/torchvggish) â€” PyTorch port of Google's VGGish audio feature extractor  
- [ECAPA-TDNN paper](https://arxiv.org/abs/2005.07143) â€” Emphasized Channel Attention, Propagation and Aggregation in TDNN  
- [ArcFace paper](https://arxiv.org/abs/1801.07698) â€” Additive Angular Margin Loss for Deep Face Recognition  
- [Speaker Verification â€“ Short Technical Notes](Speaker%20Verification%20%E2%80%93%20Short%20Technical%20Notes.pdf) â€” included in this repository  
- Hadsell et al. (2006) â€” Contrastive Loss ([paper](http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf))
